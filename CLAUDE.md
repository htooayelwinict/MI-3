# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

**Production Status**: âœ… Ready for deployment  
**Version**: 1.0.0  
**Last Updated**: August 2024

## Project Overview
MI-3 is a Python-based news scraper that automates extraction of news articles from Yahoo Finance and Google News using headless browser automation with Selenium. The project includes robust logging, debugging utilities, and dynamic selector management.

## Core Development Commands

### Setup and Dependencies
```bash
# Basic scraping functionality
pip install -r requirements-base.txt

# Full functionality with sentiment analysis
pip install -r requirement.txt

# Or install components separately
pip install -r requirements-base.txt -r requirements-ml.txt
```

### Running the Application
```bash
# Run all scrapers (default)
python main.py

# Run specific scrapers
python main.py --yahoo
python main.py --google
python main.py --all

# Update selectors before scraping (requires debugger)
python main.py --update-selectors

# Run sentiment analysis and deduplication on scraped data
python main.py --process-sentiment

# Combine scraping with sentiment processing
python main.py --all --process-sentiment

# Run debug utility to analyze website structures
python main.py --debug
```

### Individual Scraper Commands
```bash
# Yahoo Finance scraper only
python yai_scraper.py
python yai_scraper.py --update-selectors

# Google News scraper only
python ai_scraper.py

# Debug utility
python scraper_debug.py
```

### Testing and Development
```bash
# Test specific Yahoo scraper functionality
python test_yahoo_scraper.py
```

## Architecture and Key Components

### Entry Point
- `main.py`: Unified command-line interface for running all scrapers with argument parsing, result aggregation, and summary reporting

### Scraper Modules
- `yai_scraper.py`: Yahoo Finance news scraper with dynamic selector loading, cookie handling, and retry logic
- `ai_scraper.py`: Google News scraper (NewsSearcher class) with stealth browser automation
- `thllm_processor.py`: Sentiment analysis and deduplication processor using Trading-Hero-LLM model
- `scraper_debug.py`: Debug utility for analyzing website structures and extracting/updating dynamic selectors
- `selector_utils.py`: CSS selector sanitization utilities for handling complex selectors with escaping

### Data Flow
1. **Dynamic Selector Loading**: Scrapers load selectors from `debug/selectors/*.json` files if available, falling back to hardcoded defaults
2. **Browser Automation**: Selenium with stealth mode to avoid detection, Chrome headless by default
3. **Content Processing**: Scroll-based content loading, cookie consent handling, article extraction with metadata
4. **Output Generation**: Timestamped JSON files in `data/scraped_data/` directory
5. **Sentiment Processing**: Optional post-processing with sentiment analysis and deduplication, output to `data/processed_data/processed_data.json`

### Directory Structure
- `data/scraped_data/`: Output JSON files with scraped articles
- `data/processed_data/`: Sentiment-analyzed and deduplicated data (`processed_data.json`)
- `logs/`: Application logs (main.log, yai_scraper.log, ai_scraper.log, thllm_processor.log)
- `debug/html/`: Saved HTML snapshots from debug sessions
- `debug/selectors/`: Dynamic selector JSON files generated by debugger

## Important Implementation Details

### Selector Management System
- **Dynamic Loading**: Scrapers automatically load the most recent selector files from debug directory
- **Fallback Strategy**: Default selectors are embedded in code as backup
- **Sanitization**: All selectors are processed through `sanitize_selector()` to handle special characters
- **Validation**: Selector files are validated before use with error handling

### Browser Automation Patterns
- **Stealth Configuration**: Applied to avoid bot detection with platform-specific settings
- **Scroll Loading**: Incremental scrolling to trigger dynamic content loading
- **Cookie Handling**: Automated cookie consent acceptance with multiple selector fallbacks
- **Retry Logic**: Built-in retry mechanism with selector updating on failure

### Sentiment Processing System
- **Model**: Uses fuchenru/Trading-Hero-LLM for financial news sentiment analysis
- **Deduplication**: Removes duplicate articles while keeping the most recent entry per title
- **Incremental Processing**: Only processes new articles not already in processed database
- **Output Format**: Structured JSON with title, sentiment (positive/neutral/negative), timestamp, link, and source
- **Resource Management**: Model loaded on-demand with graceful fallback to neutral sentiment

### Logging Strategy
- **Module-specific logs**: Each major component has its own log file
- **Debug granularity**: Configurable logging levels with detailed debug information
- **Error tracking**: Comprehensive exception logging with stack traces

## Development Workflow
1. Use `main.py --debug` to analyze website changes and update selectors
2. Run scrapers with `--update-selectors` flag for automatic selector refreshing
3. Use `--process-sentiment` flag to add sentiment analysis to scraped data
4. Check logs in `logs/` directory for debugging scraping issues
5. Review generated selector files in `debug/selectors/` for troubleshooting
6. Monitor `data/processed_data/processed_data.json` for sentiment analysis results

## Dependencies
- **Core**: selenium, selenium-stealth, requests
- **HTML Processing**: BeautifulSoup (in debugger)
- **Sentiment Analysis**: torch (>=2.0.0), transformers (>=4.20.0)
- **Data**: json, python-dotenv
- **Browser**: Chrome/Chromium with ChromeDriver (managed by selenium)

### Installation Notes
- **Basic scraping**: `pip install -r requirement.txt` (without ML dependencies)
- **Full functionality**: Requires torch and transformers for sentiment analysis
- **Model size**: fuchenru/Trading-Hero-LLM requires ~500MB download and substantial RAM

## Error Handling Patterns
- Graceful degradation when selectors fail
- Automatic selector updating through debug utility
- Comprehensive logging of all failure modes
- Safe fallback selectors for critical functionality